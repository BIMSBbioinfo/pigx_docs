<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <title></title>
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="description" content="">
    <link rel="stylesheet" href="gitbook/style.css">
    <link rel="stylesheet" href="gitbook/website.css">
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
  </head>
  <body>
    <div class="book">
      <div class="book-summary">
        <div id="book-search-input" role="search">
          <input type="text" placeholder="Type to search" />
        </div>
        
        <nav class="summary" id="TOC" role="doc-toc">
          <h1 id="summary">Summary</h1>
          <ul>
          <li><a href="README.html">Introduction</a></li>
          <li><a href="pigx-rna-seq.html">PiGx RNA-seq</a>
          <ul>
          <li><a href="pigx-rna-seq.html#introduction">Introduction</a></li>
          <li><a href="pigx-rna-seq.html#installation">Installation</a></li>
          <li><a href="pigx-rna-seq.html#quick-start">Quick start</a></li>
          <li><a href="pigx-rna-seq.html#preparing-the-input">Preparing the input</a></li>
          <li><a href="pigx-rna-seq.html#running-the-pipeline">Running the pipeline</a></li>
          <li><a href="pigx-rna-seq.html#output-description">Output description</a></li>
          <li><a href="pigx-rna-seq.html#troubleshooting">Troubleshooting</a></li>
          <li><a href="pigx-rna-seq.html#questions">Questions</a></li>
          </ul></li>
          <li><a href="pigx-chip-seq.html#pigx-chip-seq">PiGx ChIP-seq</a>
          <ul>
          <li><a href="pigx-chip-seq.html#introduction">Introduction</a></li>
          <li><a href="pigx-chip-seq.html#installation">Installation</a></li>
          <li><a href="pigx-chip-seq.html#quick-start">Quick Start</a></li>
          <li><a href="pigx-chip-seq.html#preparing-input">Preparing Input</a></li>
          <li><a href="pigx-chip-seq.html#running-the-pipeline">Running the Pipeline</a></li>
          <li><a href="pigx-chip-seq.html#output-description">Output Description</a></li>
          <li><a href="pigx-chip-seq.html#troubleshooting">Troubleshooting</a></li>
          <li><a href="pigx-chip-seq.html#questions">Questions</a></li>
          </ul></li>
          <li><a href="pigx-bs-seq.html">PiGx BS-seq</a>
          <ul>
          <li><a href="pigx-bs-seq.html#introduction">Introduction</a></li>
          <li><a href="pigx-bs-seq.html#installation">Installation</a></li>
          <li><a href="pigx-bs-seq.html#quick-start">Quick start</a></li>
          <li><a href="pigx-bs-seq.html#preparing-the-input">Preparing the input</a></li>
          <li><a href="pigx-bs-seq.html#running-the-pipeline">Running the pipeline</a></li>
          <li><a href="pigx-bs-seq.html#output-description">Output description</a></li>
          <li><a href="pigx-bs-seq.html#troubleshooting">Troubleshooting</a></li>
          <li><a href="pigx-bs-seq.html#questions">Questions</a></li>
          </ul></li>
          <li><a href="pigx-scrna-seq.html">PiGx scRNA-seq</a>
          <ul>
          <li><a href="pigx-scrna-seq.html#introduction">Introduction</a></li>
          <li><a href="pigx-scrna-seq.html#installation">Installation</a></li>
          <li><a href="pigx-scrna-seq.html#quick-start">Quick start</a></li>
          <li><a href="pigx-scrna-seq.html#preparing-the-input">Preparing the input</a></li>
          <li><a href="pigx-scrna-seq.html#running-the-pipeline">Running the pipeline</a></li>
          <li><a href="pigx-scrna-seq.html#output-description">Output description</a></li>
          <li><a href="pigx-scrna-seq.html#troubleshooting">Troubleshooting</a></li>
          <li><a href="pigx-scrna-seq.html#questions">Questions</a></li>
          </ul></li>
          <li><a href="pigx-sars-cov-2.html">PiGx SARS-CoV-2</a>
          <ul>
          <li><a href="pigx-sars-cov-2.html#introduction">Introduction</a></li>
          <li><a href="pigx-sars-cov-2.html#installation">Installation</a></li>
          <li><a href="pigx-sars-cov-2.html#quick-start">Quick start</a></li>
          <li><a href="pigx-sars-cov-2.html#preparing-the-input">Preparing the input</a></li>
          <li><a href="pigx-sars-cov-2.html#running-the-pipeline">Running the pipeline</a></li>
          <li><a href="pigx-sars-cov-2.html#output-description">Output description</a></li>
          <li><a href="pigx-sars-cov-2.html#troubleshooting">Troubleshooting</a></li>
          </ul></li>
          </ul>
        </nav>
      </div>

      <div class="book-body">
        <div class="body-inner">
          <div class="book-header" role="navigation">
            <!-- Title -->
            <h1>
              <i class="fa fa-circle-o-notch fa-spin"></i>
              <a href="." ></a>
            </h1>
          </div>
          <div class="page-wrapper" tabindex="-1" role="main">
            <div class="page-inner">
              <div id="book-search-results">
                <div class="search-noresults">
                  <section class="normal markdown-section">
                    <h1 id="pigx-sars-cov-2">PiGx SARS-CoV-2</h1>
                    <h1 id="introduction">Introduction</h1>
                    <p>PiGx SARS-CoV-2 is a pipeline for detecting viral lineages in sequencing data obtained from enriched wastewater samples. It was developed with SARS-CoV-2 as its target virus, but other targets are theoretically possible. The viral lineages are provided by the user together with their characteristic signature mutations. The pipeline is very flexible, allowing the user to choose from multiple input and output files, and giving them fine control about parameters used by the individual tools. PiGx SARS-CoV-2 has been developed with a focus on reproucible outputs, and it can be used for continuous sampling. The output of the PiGx SARS-CoV-2 pipeline is summarized in a report which provides an intuitive visual overview about the development of lineage abundance and single significantly increasing mutations over time and location. In addition, the pipeline will generate more detailed reports per sample, which cover the quality control of the samples, the detected variants, and a taxonomic classification of all unaligned reads. This version of the pipeline was designed to work with paired-end amplicon sequencing data e.g. following the ARtIc protocols <a href="https://github.com/artic-network/artic-ncov2019/tree/master/primer_schemes/nCoV-2019/V3">ARTIC nCoV-2019 primers</a>, but single-end sequencing reads are supported as well.</p>
                    <h2 id="workflow">Workflow</h2>
                    <p>In the first step the pipeline takes the raw reads and the additional information about used primers and adapters to perform extensive quality control. Primer trimming is done with <a href="https://github.com/andersen-lab/ivar">iVAR</a>, and <a href="https://github.com/OpenGene/fastp">fastp</a> is used for adapter trimming and quality filtering. Next, the trimmed reads are aligned to the reference genome of SARS-CoV-2 using <a href="https://github.com/lh3/bwa">BWA</a>, and the results are <em>SAM</em>/<em>BAM</em> files of <strong>aligned</strong> and <strong>unaligned reads</strong>. Following the alignment a quality check on raw and processed reads is performed by using <a href="https://multiqc.info/">MultiQC</a>. Furthermore samples are checked for genome coverage and how many of the provided signature mutation sites are covered. Based on this every samples gets a quality score. Samples with genome coverage below a user defined percentage threshold are reported as discarded samples, as they are not included in time series analysises and summaries.</p>
                    <p>Calling the variants and inferring single nucleotide polymorphisms (SNVs) on the <strong>aligned reads</strong> is done with <a href="https://csb5.github.io/lofreq/">LoFreq</a>. Mutations are annotated with <a href="https://covid-19.ensembl.org/index.html">VEP</a>. Estimation of lineage frequencies is done by deconvolution (see Methods in the realted publication for details). To investigate the abundance of RNA matching other existing species in the wastewater samples the <strong>unaligned reads</strong> will be taxonomicly classified with <a href="https://github.com/DerrickWood/kraken2">Kraken2</a>. Kraken2 requires a database downloaded locally of the genomes against the reads are getting aligned. For documentation how to set this up, see: <a href="#preparing-the-databases">Prepare databases</a>. For a better and interactive visualization of all species present in the wastewater <a href="https://github.com/marbl/Krona/wiki">Krona</a> is used. Also here a small step of setting up a database is needed before running the pipeline, see: <a href="#preparing-the-databases">Prepare databases</a>. Interactive reports are generated using <a href="https://rmarkdown.rstudio.com/">R-markdown</a> and <a href="https://plotly.com/r/">plotly for R</a> for visualizations.</p>
                    <h3 id="pooling-of-samples-for-time-series-analysis-and-plots">Pooling of samples for time series analysis and plots</h3>
                    <p>For summarizing across daytime and location, the lineage frequencies are pooled by calculating the weighted average using the total number of reads of each sample as weights (missing samples are removed).</p>
                    <h2 id="output">Output</h2>
                    <ul>
                    <li>Overview reports including:
                    <ul>
                    <li>Summary and visualization of the development of SARS-CoV-2 variants and mutations over time and locations from all samples provided.</li>
                    <li>Quality Control reports per sample.</li>
                    <li>Per sample variant report: variant analysis of SARS-CoV-2 from each wastewater sample and identification of variants of concern.</li>
                    <li>Taxonomic classification of unaligned reads: Overview over taxa inferred from all the sequencing reads not aligning to the reference genome.</li>
                    </ul></li>
                    <li>Deconvoluted variant abundances</li>
                    <li>Mutation abundances</li>
                    <li>Single nucleotide variation call files</li>
                    <li>VEP reports</li>
                    <li>Kraken2 taxonomic classifications</li>
                    <li>Numerous intermediate read, alignment, and statistics files</li>
                    <li>Log files for all major analysis steps performed by the pipeline</li>
                    </ul>
                    <h1 id="installation">Installation</h1>
                    <p>Pre-built binaries for PiGx are available through <a href="https://gnu.org/s/guix">GNU Guix</a>, the functional package manager for reproducible, user-controlled software management. You can install the PiGx SARS-CoV-2 pipeline with</p>
                    <div class="sourceCode" id="cb1"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">guix</span> install pigx-sars-cov-2</span></code></pre></div>
                    <p>If you want to install PiGx SARS-CoV-2 from source, please clone this repository and change directory accordingly:</p>
                    <div class="sourceCode" id="cb2"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/BIMSBbioinfo/pigx_sars-cov-2.git</span>
                    <span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> pigx_sars-cov-2</span></code></pre></div>
                    <p>To fetch code that is common to all PiGx pipelines run this:</p>
                    <div class="sourceCode" id="cb3"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> submodule update <span class="at">--init</span></span></code></pre></div>
                    <p>Before setting everything up, though, make sure <a href="https://github.com/BIMSBbioinfo/pigx_sars-cov-2/blob/main/manifest.scm">all dependencies</a> are met by either installing them manually, or by entering the provided reproducible Guix environment. If you are using Guix we definitely recommend the latter. This command spawns a sub-shell in which all dependencies are available at exactly the same versions that we used to develop the pipeline:</p>
                    <div class="sourceCode" id="cb4"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="va">USE_GUIX_INFERIOR</span><span class="op">=</span>t <span class="ex">guix</span> environment <span class="at">--pure</span> <span class="at">-m</span> manifest.scm <span class="at">--preserve</span><span class="op">=</span>GUIX_LOCPATH</span></code></pre></div>
                    <p>To use your current Guix channels instead of the fixed set of channels, just omit the <code>USE_GUIX_INFERIOR</code> shell variable:</p>
                    <div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">guix</span> environment <span class="at">--pure</span> <span class="at">-m</span> manifest.scm <span class="at">--preserve</span><span class="op">=</span>GUIX_LOCPATH</span></code></pre></div>
                    <p>Note that <code>--pure</code> unsets all environment variables that are not explicitly preserved. To access other executables that are not part of the environment please address them by their absolute file name.</p>
                    <p>Inside the environment you can then perform the usual build steps:</p>
                    <div class="sourceCode" id="cb6"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">./bootstrap.sh</span> <span class="co"># to generate the &quot;configure&quot; script</span></span>
                    <span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./configure</span></span>
                    <span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span></span>
                    <span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="fu">make</span> check</span></code></pre></div>
                    <p>At this point you are able to run PiGx SARS-CoV-2. To see all available options type <code>--help</code>.</p>
                    <div class="sourceCode" id="cb7"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pigx-sars-cov-2</span> <span class="at">--help</span></span></code></pre></div>
                    <h2 id="preparing-the-databases">Preparing the databases</h2>
                    <p>Before the pipeline can work, three databases must be downloaded to a location specified in the settings file. Depending on the size of the databases this can take some time.</p>
                    <p>Without any user intervention, this will happen automatically via snakemake rules. This behaviour is controlled via parameters in the settings file. See the <a href="#settings-file">Settings file</a> section for details.</p>
                    <p>Alternatively, the databases may be downloaded manually via the <code>download_databases.sh</code> scripts accessible like so:</p>
                    <div class="sourceCode" id="cb8"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="va">prefix</span><span class="op">=</span><span class="st">&quot;</span><span class="va">$(</span><span class="fu">dirname</span> pigx-sars-cov-2<span class="va">)</span><span class="st">/../&quot;</span></span>
                    <span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="va">$prefix</span><span class="ex">/libexec/pigx_sars-cov-2/scripts/download_databases.sh</span></span></code></pre></div>
                    <p>However, the <code>download_databases.sh</code> script does not offer the flexibility of downloading the databases automatically with user defined parameters, unless it is manually edited.</p>
                    <p><em>Note: The directory that the databases will be downloaded to needs to match the database directories given in the settings file, else the pipeline will download the databases to the given directory again, unnecessarily using up space. This should be the case by default though.</em></p>
                    <p>Read on for details if you want to download the databases manually.</p>
                    <h3 id="kraken2-database">Kraken2 database</h3>
                    <p>There are several libraries of genomes that can be used to classify the (unaligned) reads. It is up to you which one to use, but be sure that they fulfill the necessities stated by Kraken2 (<a href="https://github.com/DerrickWood/kraken2/wiki/Manual#kraken-2-databases">Kraken2 manual</a>). For an overall overview we recommend to use the Plus-PFP library provided <a href="https://benlangmead.github.io/aws-indexes/k2">here</a>, which is also the default library used in the pipeline. If the classification is not of concern or only the viruses are of interest, we recommend using a smaller one. This will speed up the pipeline.</p>
                    <p>After downloading and unpacking the database files, use <code>kraken2-build</code> to download the taxonomy data and build the database.</p>
                    <h3 id="krona-database">Krona database</h3>
                    <p>The way we use Krona, we only need the taxonomy database, as downloaded via their <code>updateTaxonomy.sh</code> script.</p>
                    <h3 id="vep-database">VEP database</h3>
                    <p>For our use of VEP in the pipeline, we need a pre-indexed cache of the VEP database of transcript models. This is the main point of the pipeline that determines which virus can be analysed with the pipeline. Currently, VEP only has data on SARS-CoV-2. But the VEP cache is the only point strictly determining which virus the pipeline can deal with.</p>
                    <p>Per default the pipeline uses the indexed cache archive at <code>http://ftp.ensemblgenomes.org/pub/viruses/variation/indexed_vep_cache/sars_cov_2_vep_101_ASM985889v3.tar.gz</code>, which only needs to be unpacked to the target directory. Currently this is the only available chache file.</p>
                    <h1 id="quick-start">Quick start</h1>
                    <p>To check whether the pipeline and the databases have been properly set up, run the pipeline on a minimal test dataset. If you installed the pipeline from source, start at step 3.</p>
                    <ol type="1">
                    <li><p>Download the test data</p>
                    <p><code>git clone https://github.com/BIMSBbioinfo/pigx_sars-cov-2 sarscov2-test</code></p></li>
                    <li><p>Enter the directory</p>
                    <p><code>cd sarscov2-test</code></p></li>
                    <li><p>Run the pipeline using a preconfigured settings file. If you installed the pipeline from source, the database location will be set to whatever dir was specified during the configure step. If you installed the databases manually, you will need to also adjust the database paths in the test settings file accordingly.</p>
                    <p><code>pigx-sars-cov-2 -s tests/setup_test_settings.yaml tests/sample_sheet.csv</code></p></li>
                    </ol>
                    <p>Inside <code>tests/</code> a new directory <code>output_setup_test</code> is created, which includes specific directories containing output data for the respective step of the pipeline. The <code>tests/output_setup_test/reports/index.html</code> gives the overview over all merged reports for the test data.</p>
                    <h1 id="preparing-the-input">Preparing the input</h1>
                    <p>In order to run the pipeline, you need to supply</p>
                    <ul>
                    <li>Sample sheet (CSV format): containing information about sampling date and<br />
                    location</li>
                    <li>Settings file (YAML format) for specifying the experimental setup and optional custom parameter adjustments</li>
                    <li>Mutation sheet containing the lineages of interest and their signature mutations in nucleotide notation (CSV format)</li>
                    <li>Mutation BED file containing the genomic coordinates of the mutation sites (see below for details)</li>
                    <li>Reference genome of the target species in fasta format (so far the pipeline is only optimized for SARS-CoV-2, others might work too but not yet<br />
                    tested)</li>
                    <li>Primer BED file containing the PCR primer locations (e.g the primers suggested from ARTIC protocols)</li>
                    </ul>
                    <p>In order to generate template settings and sample sheet files, type</p>
                    <div class="sourceCode" id="cb9"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pigx-sars-cov-2</span> <span class="at">--init</span></span></code></pre></div>
                    <p>in the shell, and a boilerplate <code>sample_sheet.csv</code> and <code>settings.yaml</code> will be written to your current directory. An example for both files is provided in the <code>tests/</code> directory.</p>
                    <h2 id="sample-sheet">Sample sheet</h2>
                    <p>The sample sheet is a tabular file (<code>csv</code> format) describing the experiment. The table has the following columns:</p>
                    <table>
                    <thead>
                    <tr class="header">
                    <th>SampleName</th>
                    <th>Read</th>
                    <th>Read2</th>
                    <th>date</th>
                    <th>location_name</th>
                    <th>coordinates_lat</th>
                    <th>coordinates_long</th>
                    </tr>
                    </thead>
                    <tbody>
                    <tr class="odd">
                    <td>Test0</td>
                    <td>Test0_R1.fastq</td>
                    <td>Test0_R2.fastq</td>
                    <td>2021-01-01T08:00:00</td>
                    <td>Berlin</td>
                    <td>52.364</td>
                    <td>13.509</td>
                    </tr>
                    <tr class="even">
                    <td>Test2</td>
                    <td>Test2_R1.fastq</td>
                    <td>Test2_R2.fastq</td>
                    <td>2021-01-03T08:00:00</td>
                    <td>Munich</td>
                    <td>48.208</td>
                    <td>11.628</td>
                    </tr>
                    </tbody>
                    </table>
                    <ul>
                    <li><em>SampleName</em> is the name for the sample</li>
                    <li><em>Read</em> &amp; <em>Read2</em> are the fastq file names of paired end reads
                    <ul>
                    <li>the location of these files is specified in the settings file</li>
                    <li>in the case of single-end data, leave the <code>Read2</code> column <strong>empty</strong></li>
                    </ul></li>
                    <li><em>date</em> is a date/time in ISO format (<code>yyyy-mm-ddThh:mm:ss</code>)</li>
                    <li><em>location_name</em> is the name of the location and should be unique per coordinates</li>
                    <li><em>coordinates_lat</em> &amp; <em>coordinates_long</em> correspond the latitude and longitude of the location name</li>
                    </ul>
                    <h2 id="mutation-sheet">Mutation sheet</h2>
                    <p>The mutation sheet should contain one column of siganture mutations per lineage that shall be tracked and analysed by deconvolution. They should be given in the format <code>GENE:RxxxV</code>, where <code>GENE</code> is the name of the gene in which the mutation is found, <code>R</code> is a string of reference nucleotides in upper case, <code>xxx</code> is the first reference nucleotides position (a number), and <code>V</code> is a string of variant nucleotides, also in upper case. There is no upper or lower limit for the number of signature mutations per lineage. However, please note that the deconvolution results are more robust and precise with a higher number of mutations. (Tested with 10-30 mutations per lineage).</p>
                    <h2 id="mutation-bed-file">Mutation BED file</h2>
                    <p>The BED file for testing if the mutation sites are covered should have 4<br />
                    columns:</p>
                    <ol type="1">
                    <li>chromosome name</li>
                    <li>start - 5bp <strong>before</strong> the mutation location</li>
                    <li>end - 5bp <strong>after</strong> the mutation location</li>
                    <li>name with the original location of the mutation in the format of: name_MutationLocation_name, e.g: “nCoV-2019_210_SigmutLocation” A row in the BED file should look like:<br />
                    <code>NC_045512.2 205 215 nCoV-2019_210_SigmutLocation</code><br />
                    Please see the example file within the test directory for a detailed example.</li>
                    </ol>
                    <h2 id="primer-bed-file">Primer BED file</h2>
                    <p>The primer file contains the locations of primer sequences on the reads, i.e. it defines where in the genomes primer sequences <em>may</em> occurr. This is determined by the primer scheme used in generating the sequencing reads. It is required by iVar for primer trimming. An example file can be found in the test directory (<code>nCoV-2019_NCref.bed</code>).</p>
                    <h2 id="settings-file">Settings file</h2>
                    <p>The settings file contains parameters (in YAML format) to configure the execution of the PiGx SARS-CoV-2 pipeline. There are generally two settings files at play:</p>
                    <ol type="1">
                    <li>The pipeline interal settings file (source: <code>etc/settings.yaml</code>, installed: <code>$prefix/share/pigx_sars-cov-2/settings.yaml</code>), containing default settings. This file is not supposed to be modified by the user.</li>
                    <li>An analysis specific, user provided settings file containing changes to the default settings. These are generally paths to the input files, databases, etc.</li>
                    </ol>
                    <p>When the pipeline is executed, both settings files are combined into a run specific config file (<code>config.json</code>), used by the internal <code>snakemake</code> workflow manager. It is always generated in the place the <code>pigx-sars-cov-2</code> program was called and will be overwritten on subsequent calls.</p>
                    <details>
                      <summary>Detailed settings explanation.</summary>

                    <h3 id="locations"><code>locations</code></h3>
                    <p>Paths to various input files and directories needed by the pipeline.</p>
                    <ul>
                    <li><em>output-dir</em> output directory for the pipeline.</li>
                    <li><em>input-dir</em> direcotry containing the input files, the files therin should match the file suffix given under control/start.</li>
                    <li><em>reference-fasta</em> <a href="#mutation-sheet">Mutation sheet</a></li>
                    <li><em>primers-bed</em> <a href="#primer-bed-file">Primer BED file</a></li>
                    <li><em>mutations-bed</em> <a href="#mutation-bed-file">Mutation BED file</a></li>
                    <li><em>mutation-sheet</em> <a href="#mutation-sheet">Mutation table</a></li>
                    <li><em>kraken-db-dir</em> <a href="#kraken2-database">Kraken2 database</a></li>
                    <li><em>krona-db-dir</em> <a href="#krona-database">Krona database</a></li>
                    <li><em>vep-db-dir</em> <a href="#vep-database">VEP database</a></li>
                    </ul>
                    <h3 id="databases"><code>databases</code></h3>
                    <p>Settings controlling the download and subsequent processing of databases needed for the pipeline. If the databases are already present, none of these settings will have any effect.</p>
                    <p>When prebuilt archives are to be used, the pipeline can download them both via <code>ftp</code> and <code>http</code>/<code>https</code>. If no protocol is included, <code>http</code>/<code>https</code> is assumed.</p>
                    <p><em>Note: Generally the official databases will be used except when running <code>make check</code>/<code>make distcheck</code> without the databases pre-installed at the default location. In that case the settings file at <code>tests/settings.yaml</code> will be used, which configures the pipeline to use database archives prebuilt by us. This is to ensure the github actions can run smoothly. In order to build the archives, at least 1GB of disk space is necessary, which is not given on a github action runner.</em></p>
                    <h4 id="kraken2"><code>kraken2</code></h4>
                    <p>The kraken2 database download is the most complex of the three database downloads. The download is done via one of the database archive file provided at the <a href="https://benlangmead.github.io/aws-indexes/k2">Index Zone</a>, except in the case outlined above.<br />
                    When no downsampling should occur, the database archive is extracted and used as-is. In the other case, everything except the hash file (<code>hash.k2d</code>) is extracted, the taxonomy is downloaded, and the database is built on disk. The taxonomy files are removed afterwards as they are not used again and only take up space after database building, at least for our purposes.</p>
                    <h5 id="archive-url"><code>archive-url</code></h5>
                    <p>The url the kraken2 archive will be downloaded from. The default archive is the very large database including protozoa and fungi in addition to the standard database.</p>
                    <p>When prebuilt archives are to be used, the pipeline can download them both via <code>ftp</code> and <code>http</code>/<code>https</code>. If no protocol is included, <code>http</code>/<code>https</code> is assumed.</p>
                    <p>Default: <a href="https://genome-idx.s3.amazonaws.com/kraken/k2_pluspfp_20210127.tar.gz">https://genome-idx.s3.amazonaws.com/kraken/k2_pluspfp_20210127.tar.gz</a></p>
                    <h5 id="downsample-db"><code>downsample-db</code></h5>
                    <p>Whether or not downsampling of the <code>hash.k2d</code> file should occurr.</p>
                    <p>Default: false</p>
                    <h5 id="max-db-size-bytes"><code>max-db-size-bytes</code></h5>
                    <p>If downsampling should occurr, what is the maximum allowable size in bytes? The resulting file may be smaller than the given value. The value is passed on directly to <code>kraken2-build</code> under the <code>--max-db-size</code> option. Will only have an effect of <code>downsample_db</code> is true.</p>
                    <p>Default: 250000000</p>
                    <h4 id="krona"><code>krona</code></h4>
                    <p>The Krona database download is fairly simple. Either the Krona internal download script <code>updateTaxnomy.sh</code> (<a href="https://github.com/marbl/Krona/wiki/Installing">Documentation</a>) is used, or a prebuilt archive is downloaded.</p>
                    <h5 id="use-prebuilt"><code>use-prebuilt</code></h5>
                    <p>Whether or not a prebuilt archive should be downloaded instead of running the update taxonomy script.</p>
                    <p>Default: false</p>
                    <h5 id="archive-url-1"><code>archive-url</code></h5>
                    <p>If a prebuilt archive should be downloaded, this tells the pipeline where to find it.</p>
                    <p>When prebuilt archives are to be used, the pipeline can download them both via <code>ftp</code> and <code>http</code>/<code>https</code>. If no protocol is included, <code>http</code>/<code>https</code> is assumed.</p>
                    <p>Default: ““</p>
                    <h4 id="vep"><code>vep</code></h4>
                    <p>The vep download is the simplest. The database is always in a compressed archive.</p>
                    <h5 id="archive-url-2"><code>archive-url</code></h5>
                    <p>Location from where the archive will be downloaded.</p>
                    <p>When prebuilt archives are to be used, the pipeline can download them both via <code>ftp</code> and <code>http</code>/<code>https</code>. If no protocol is included, <code>http</code>/<code>https</code> is assumed.</p>
                    <p>Default: <a href="ftp://ftp.ensemblgenomes.org/pub/viruses/variation/indexed_vep_cache/sars_cov_2_vep_101_ASM985889v3.tar.gz">ftp://ftp.ensemblgenomes.org/pub/viruses/variation/indexed_vep_cache/sars_cov_2_vep_101_ASM985889v3.tar.gz</a></p>
                    <h3 id="parameters"><code>parameters</code></h3>
                    <h4 id="vep-1"><code>vep</code></h4>
                    <p>Parameters for rule vep. See documentation about vep arguments <code>--species</code>, <code>--buffer_size</code>, and <code>--distance</code> respectively in the vep <a href="https://grch37.ensembl.org/info/docs/tools/vep/script/vep_options.html">documentation</a>.</p>
                    <h5 id="species"><code>species</code></h5>
                    <p>Needs to match with the downloaded VEP database cache. The <code>vep</code> default is “homo_sapiens”, which is unlikely to be of use for this pipeline.</p>
                    <p>Directly passed to the <code>vep</code> parameter <code>--species</code>.</p>
                    <p>Default: sars_cov_2</p>
                    <h5 id="buffer-size"><code>buffer-size</code></h5>
                    <p>Number of variants in memory at one time. Trades off between run time and memory usage. The higher, the faster.</p>
                    <p>Directly passed to the <code>vep</code> parameter <code>--buffer-size</code>.</p>
                    <p>Default: 5000</p>
                    <h5 id="transcript-distance"><code>transcript-distance</code></h5>
                    <p>Up- and downstream distance of a gene and a transcript which will be classified as an up- or downstream variant.</p>
                    <p>Directly passed to the <code>vep</code> parameter <code>--distance</code>.</p>
                    <p>Default: 5000</p>
                    <h5 id="db-version"><code>db-version</code></h5>
                    <p>This specifies a database version, which is needed when the database version differs from the <code>vep</code> executable version. By default, <code>vep</code> looks for a database version matching its own. This is needed even when using a offline database cache like this pipeline does. Therefore this needs to be adjusted when changing the <code>vep</code> database used.</p>
                    <p>Directly passed to the <code>vep</code> parameter <code>--distance</code>.</p>
                    <p>Default: 101</p>
                    <h4 id="ivar_trimming"><code>ivar_trimming</code></h4>
                    <p>Parameters for rule <code>ivar_primer_trim</code>. See documentation about ivar arguments <code>-q</code>, <code>-m</code>, and <code>-s</code> respectively in the iVar <a href="https://andersen-lab.github.io/ivar/html/manualpage.html">documentation</a>.</p>
                    <p><code>ivar</code> trimms primers from alignment BED files by first removing the sections listed in a second primer BED file, and then performing quality trimming by sliding a window along each read from 5’ to 3’ end, clipping the read if the average window quality drops below a given cutoff.</p>
                    <h5 id="quality-cutoff"><code>quality-cutoff</code></h5>
                    <p>If the average base call quality in the sliding window drops below this value, the read will be trimmed to the last base of sufficient average window quality.</p>
                    <p>Directly passed to the <code>ivar</code> parameter <code>-q</code>.</p>
                    <p>Default: 15</p>
                    <h5 id="length-cutoff"><code>length-cutoff</code></h5>
                    <p>Read length threshold, if a read is shorter than this after trimming, it will be discarded.</p>
                    <p>Directly passed to the <code>ivar</code> parameter <code>-m</code>.<br />
                    Default: 30</p>
                    <h5 id="window-width"><code>window-width</code></h5>
                    <p>Number of bases in the sliding window, i.e. the window width.</p>
                    <p>Directly passed to the <code>ivar</code> parameter <code>-qs</code>.</p>
                    <p>Default: 4</p>
                    <h4 id="reporting"><code>reporting</code></h4>
                    <p>These paramerters are used to set quality control filters for the reports.</p>
                    <h5 id="mutation-coverage-threshold"><code>mutation-coverage-threshold</code></h5>
                    <p>Results from samples without sufficient coverage measures are not included in the visualizations or the linear regression calculations.</p>
                    <p>Default: 90</p>
                    <h4 id="deconvolution"><code>deconvolution</code></h4>
                    <h5 id="method"><code>method</code></h5>
                    <p>Control the deconvolution method used. Possible options are:</p>
                    <ul>
                    <li><code>rlm</code>: Robust linear regression as implemented in the <code>MASS</code> package, executed via the <code>deconvR</code> package.</li>
                    <li><code>nnls</code>: Non-negative least squares (<code>deconvR</code>).</li>
                    <li><code>qp</code>: Quadratic programming (<code>deconvR</code>).</li>
                    <li><code>svr</code>: Support vector regression (<code>deconvR</code>).</li>
                    </ul>
                    <p>Prepending “weighted_” to the chosen method will add weighting of bulk mutation abundances per variant by the inverse of the proportion of detected mutations to known mutations. This biases the abundance of variants with high proportions towards higher values, and coversely biases the abundence regression is of variants with low proportions towards lower values.</p>
                    <p>Default: weighted_rlm</p>
                    <h5 id="mutation-depth-threshold"><code>mutation-depth-threshold</code></h5>
                    <p>Minimum sequencing per mutation for it to be used in the analysis.</p>
                    <p>Default: 100</p>
                    <h2 id="control"><code>control</code></h2>
                    <p>The following settings control from which point the pipeline starts, which path it follows (i.e. which rules are executed on the way) and where it stops.</p>
                    <h5 id="start"><code>start</code></h5>
                    <p>Start points for the analysis</p>
                    <ul>
                    <li>fastq(.gz): Raw reads in (gzipped) FASTQ format</li>
                    <li>bam: Unfiltered alignments</li>
                    <li>vcf: Variant calling files. <em>Note: Ideally these should have been generated by lofreq. The minimum requirement is that the INFO fields “AF” and “DP” are present. (More details on <a href="https://samtools.github.io/hts-specs/">here</a>)</em></li>
                    </ul>
                    <p>When using a non-default target, not all rules will be able to run and it won’t be possible to generate all reports. The main reasons for this are the missing quality control statistics that are necessary mainly for the final report, and the missing unalinged read files when starting after the alignment step.</p>
                    <p>Default: fastq.gz</p>
                    <h5 id="targets"><code>targets</code></h5>
                    <p>Desired results of the analysis. Multiple targets at the same time is possible.</p>
                    <ul>
                    <li>help: Print all rules and their descriptions.</li>
                    <li>final_reports: Produce a comprehensive report. This is the default target.</li>
                    <li>devonvolution: Run deconvolution for all provided samples and create a summary table containing abundances from all samples.</li>
                    <li>lofreq: Call variants and produce .vcf file and overview .csv file.</li>
                    <li>multiqc: Create MultiQC reports for including raw and trimmed reads.</li>
                    </ul>
                    <p>Default: - final_reports</p>
                    <h5 id="run-ivar-primer-trimming"><code>run-ivar-primer-trimming</code></h5>
                    <p>Whether the primer trimming step should be performed.</p>
                    <p>Default: yes</p>
                    <h4 id="execution"><code>execution</code></h4>
                    <p>Settings directly related to the program execution itself.</p>
                    <h5 id="jobs"><code>jobs</code></h5>
                    <p>The number of jobs allowed to be executed at one time. If executed locally this specifies the maximum number of cores used at a time. May not extend to the number of jobs used by individual tools. See section <code>--cores</code> &amp; <code>--jobs</code> in the <a href="https://snakemake.readthedocs.io/en/stable/executing/cli.html#all-options">snakemake docs</a></p>
                    <p>Default: 6</p>
                    <h5 id="submit-to-cluster"><code>submit-to-cluster</code></h5>
                    <p>Whether cluster specific settings will be respected, and a qsub submission call should be executed.</p>
                    <p>Default: no</p>
                    <h5 id="cluster"><code>cluster</code></h5>
                    <p>Settings specific to executing the pipeline on a computing cluster. None of these are relevant when <code>submit-to-cluster</code> is “no”.</p>
                    <h6 id="missing-file-timeout"><code>missing-file-timeout</code></h6>
                    <p>How long before a rule output file is declared missing and the pipeline stopped.</p>
                    <p>When executing the pipeline on a cluster, file system latency can be higher than when the pipeline is executed on a single server. Therefore the time snakemake waits before declaring a rule output file as missing and stopping needs to be adjusted.</p>
                    <p>Default: 120</p>
                    <h6 id="stack"><code>stack</code></h6>
                    <p>Stack memory used for each rule. We recommend leaving it as it is, unless you really know what you are doing.</p>
                    <p>Default: 128M</p>
                    <h6 id="queue"><code>queue</code></h6>
                    <p>The name of a specific queue the whole pipeline should be submitted to.</p>
                    <p>Default: all</p>
                    <h6 id="contact-email"><code>contact-email</code></h6>
                    <p>How the cluster administration may contact you.</p>
                    <p>Default: none</p>
                    <h6 id="args"><code>args</code></h6>
                    <p>Additional arguments passed to <code>qsub</code>, as a single string.</p>
                    <p>Default: ’’</p>
                    <h5 id="rules"><code>rules</code></h5>
                    <p>Per rule submission settings. Give the rule name as a heading to configure that specific rule.</p>
                    <h6 id="__default__"><code>__default__</code></h6>
                    <p>Default settings used in absence of rule specific settings.</p>
                    <ul>
                    <li><code>threads</code>: Number of threads. Default is 1.</li>
                    <li><code>memory</code>: RAM available for the rule, with unit (e.g. “90K”, “5M”). In the case of no unit, unit M is assumed. Default is 4G.</li>
                    </ul>
                    <h3 id="tools"><code>tools</code></h3>
                    <p>Overwrites for locations of specific tools used. Each tool has its own subheading, i.e. “bwa”, and the following settings:</p>
                    <h4 id="executable"><code>executable</code></h4>
                    <p>Path to the executable file for the tool, defaults to the system installation.</p>
                    <h4 id="arguments"><code>arguments</code></h4>
                    <p>Additional arguments to the tool as one string. Only use this if you know what you are doing, these may cause conflicts with the arguments supplied in each rule.</p>
                    </details>

                    <h1 id="running-the-pipeline">Running the pipeline</h1>
                    <p>PiGx SARS-CoV-2 wastewater is executed using the command <code>pigx-sars-cov-2 -s settings.yaml sample_sheet.csv</code>. See <code>pigx-sars-cov-2 --help</code> for information about additional command line arguments.</p>
                    <p>The <code>execution</code> section of the settings file provides some control over the execution of the pipeline.</p>
                    <h2 id="local--cluster-execution">Local / cluster execution</h2>
                    <p>The workflow may be executed locally (on a single computer), or, if a Sun Grid Engine-compatible HPC environment is available, supports cluster execution. In order to enable cluster execution, specify <code>submit-to-cluster: yes</code> in the settings file.</p>
                    <h2 id="parallel-execution">Parallel execution</h2>
                    <p>If the workflow is run on a cluster, or a single computer with sufficient resources, some of the tasks may be computed in parallel. To specify the allowed level or parallelism, set the <code>jobs</code> setting under <code>execution</code> in the settings file. For instance,</p>
                    <div class="sourceCode" id="cb10"><pre class="sourceCode yaml"><code class="sourceCode yaml"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">execution</span><span class="kw">:</span></span>
                    <span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">submit-to-cluster</span><span class="kw">:</span><span class="at"> </span><span class="ch">yes</span></span>
                    <span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">jobs</span><span class="kw">:</span><span class="at"> </span><span class="dv">40</span></span></code></pre></div>
                    <p>in the settings file, will submit up to 40 simultaneous compute jobs on the cluster.</p>
                    <h1 id="output-description">Output description</h1>
                    <p>PiGx SARS-CoV-2 wastewater creates an output directory, as specified in the settings file, that contains all of the following outputs.</p>
                    <h2 id="time-series-reports">Time series reports</h2>
                    <p>This pipeline performs mutation analysis of SARS-CoV-2 and reports and quantifies the occurrence of <em>variants of concern</em> (VOC) and signature mutations by which they are characterised.</p>
                    <p>The visualizations in the time series report provide an overview of the evolution of VOCs and signature mutations found in the analyzed samples across given time points and locations. The abundance values for the variants are derived by deconvolution. The frequencies of the mutations are the output of <a href="https://csb5.github.io/lofreq/">LoFreq</a>.</p>
                    <h2 id="quality-control">Quality control</h2>
                    <p>A quality control report is generated for each sample. It includes reports on amplicon coverage and read coverage, as well as general quality control and preprocessing metrics.</p>
                    <p>General quality control metrics are computed using <a href="https://www.bioinformatics.babraham.ac.uk/projects/fastqc/">FastQC</a> and <a href="https://multiqc.info/">MultiQC</a>. The MultiQC report is particularly useful, collating quality control metrics from many steps of the pipeline in a single HTML report, which may be found under the <code>multiqc</code> directory in the PiGx output folder.</p>
                    <h2 id="taxonomic-classification">Taxonomic classification</h2>
                    <p>This report provides an overview of the species found in the provided wastewater samples apart from SARS-CoV-2. The SARS-CoV-2 enriched wastewater samples are aligned to the virus genome. It provides insight about possible biases and contamination of the samples. In case of abundance of species very similar to SARS-CoV-2 only the taxonomic family will be reported which could indicate that an identification/alignment of SARS-CoV-2 could be biased or impossible. In case of a high percentage of read matching SARS-CoV-2 a refining of trimming parameters should be considered.</p>
                    <h2 id="variant-report">Variant report</h2>
                    <p>This report shows the variant analysis of SARS-CoV-2 from wastewater samples. Mutations are identified by single-nucleotide-variant (SNV) calling performed by <a href="https://csb5.github.io/lofreq/">LoFreq</a>. Translated to amino acid mutations by using <a href="https://covid-19.ensembl.org/info/docs/tools/vep/index.html">Ensemble VEP - COVID-19</a>. The list of found mutations (including synonymous and non-synonymous mutations) were matched against lists of signature mutations characterising variants of concern (VOC) of SARS-CoV-2 provided by <a href="https://outbreak.info/situation-reports">outbreak.info</a> and <a href="https://covariants.org/variants/S.501Y.V1">CoVariant.org</a>.</p>
                    <details>
                      <summary>All outputs with their locations.</summary>

                    <p><em>Given locations are relative to the output directory. <code>SAMPLE</code> indicates a variable part of a path that will be replaced with a sample name. <code>VIRUS</code> will be replaced with the virus being investigated.</em></p>
                    <ul>
                    <li>Overview reports:
                    <ul>
                    <li>Summary and visualization of the development of SARS-CoV-2 variants and mutations over time and locations from all samples provided (<code>report/index.html</code>).</li>
                    <li>Quality Control reports per sample:
                    <ul>
                    <li>Overall QC report with number of covered amplicons, read coverage, etc (<code>report/SAMPLE.qc_report_per_sample.html</code>).</li>
                    <li>MultiQC report per sample for raw and trimmed reads along with several supplementary files (<code>report/multiqc/SAMPLE/*</code>).</li>
                    <li>FASTQC reports per sample and read for raw reads, adapter trimmed reads, and aligned reads, along with an archive of supplementary files (<code>report/fastqc/SAMPLE/*</code>).</li>
                    <li>FASTP reports per sample on adapter trimming statistics (<code>fastp/SAMPLE/*</code>).</li>
                    </ul></li>
                    <li>Per sample variant report: variant analysis of SARS-CoV-2 from each wastewater sample and identification of variants of concern (<code>report/SAMPLE.qc_report_per_sample.html</code>).</li>
                    <li>Taxonomic classification of unaligned reads: Overview over taxa inferred from all the sequencing reads not aligning to the reference genome (<code>report/SAMPLE.taxonomic_classification.html</code>).</li>
                    </ul></li>
                    <li>Alignments:
                    <ul>
                    <li>Per sample aligned reads: Reads aligned against the reference genome, at various stages of trimming and sorting, along with their index files (<code>mapped_reads/SAMPLE_aligned.(bam|sam|bai)</code>).</li>
                    <li>Per sample unaligned reads: Reads not aligning to the reference genome (<code>mapped_reads/SAMPLE_unalingned.(bam|fastq)</code>).</li>
                    </ul></li>
                    <li>SNV files:
                    <ul>
                    <li>Per sample files listing all detected single nucleotide variants (SNVs) from the aligned reads (also parsed to csv, <code>variants/SAMPLE_snv.(vcf|csv)</code>).</li>
                    </ul></li>
                    <li>VEP reports:
                    <ul>
                    <li>Per sample report files constituting the <a href="https://www.ensembl.org/info/docs/tools/vep/vep_formats.html#output">VEP output</a> including the variants uploaded to the VEP database, resulting amino acid changes and consequences for the corresponding protein.
                    <ul>
                    <li>Raw VEP output (<code>variants/SAMPLE_vep_VIRUS.txt</code>).</li>
                    <li>Report with detailed run statistics (<code>variants/SAMPLE_vep_VIRUS.txt_summary.html</code>).</li>
                    <li>Warnings generated by vep during the run (<code>variants/SAMPLE_vep_VIRUS.txt_warnings.txt</code>)</li>
                    <li>File with most of the raw output parsed into a comma separated table for downstream processing (<code>variants/SAMPLE_vep_VIRUS_parsed.csv</code>).</li>
                    </ul></li>
                    </ul></li>
                    <li>Deconvoluted variant abundances:
                    <ul>
                    <li>Per sample abundance of each of the VOCs as
                    <ul>
                    <li>a regular table (<code>variants/SAMPLE_variant_abundance.csv</code>),</li>
                    <li>in a single row together with metadata (used later to construct summary, <code>variants/SAMPLE_variants_with_meta.csv</code>).</li>
                    </ul></li>
                    <li>Overall summary table of per sample variant abundances with sample metadata (<code>variants/data_variant_plot.csv</code>).</li>
                    </ul></li>
                    <li>Mutation abundances:
                    <ul>
                    <li>Per sample abundance of each signature mutation found and meeting the read depth threshold criterium as a single row together with metadata (used later to construct summary, <code>mutations/SAMPLE_mutations.csv</code>).</li>
                    <li>Overall summary table <em>CSV</em> file of per sample mutation abundances with sample metadata.</li>
                    <li>Mutation data of non-signature mutations meeting the read depth threshold (<code>mutations/SAMPLE_non_sigmuts.csv</code>).</li>
                    </ul></li>
                    <li>Sample quality:
                    <ul>
                    <li>Per sample tables of read depth at each signature mutation locus, derived from the <em>untrimmed</em> alignment (<code>coverage/SAMPLE_genome_cov.tsv</code>).</li>
                    <li>Per sample tables giving coverage statistics of the <em>untrimmed</em> alignment (<code>coverage/SAMPLE_mut_cov.tsv</code>).</li>
                    <li>Per sample tables aggregating the statistics of genome and mutation coverage files in a sample quality table (single row for downstream use, <code>coverage/SAMPLE_quality.csv</code>).</li>
                    <li>Overall table concatenating the per sample quality rows into one table (<code>coverage/sample_quality_table.csv</code>).</li>
                    </ul></li>
                    <li>Adapter-trimmed reads (<code>trimmed_reads/SAMPLE_trimmed.fastq.gz</code>).</li>
                    <li>Kraken2 raw output: provides an overview of all found species in the unaligned reads together with the NCBI taxonomy ID (<code>kraken/SAMPLE_classified_unaligned_reads.txt</code>).</li>
                    <li>Per sample Krona diagram of taxa proportions in unaligned reads (<code>report/SAMPLE.Krona_report.html</code>).</li>
                    <li>Mutation count summary table containing count statistics for mutations overall and per sample (<code>mutation_counts.csv</code>).</li>
                    <li>Raw table of per mutation linear model coefficients and their p-values, generated by the <code>mutation_regression.R</code> script (<code>unfiltered_mutations_sig.csv</code>).</li>
                    <li>Sample summary table containing various statistics about each sample (<code>overview_QC.csv</code>).</li>
                    <li>Log files for all major analysis steps performed by the pipeline (<code>logs/*.log</code>).</li>
                    </ul>
                    </details>

                    <h1 id="troubleshooting">Troubleshooting</h1>
                    <p>If you have any questions please e-mail: <a href="mailto:pigx@googlegroups.com">pigx@googlegroups.com</a> or use the web form to ask questions <a href="https://groups.google.com/forum/#!forum/pigx/">https://groups.google.com/forum/#!forum/pigx/</a>.</p>
                    <p>If you run into any bugs, please open an issue here: <a href="https://github.com/BIMSBbioinfo/pigx_rnaseq/issues">https://github.com/BIMSBbioinfo/pigx_rnaseq/issues</a>.</p>
                  </section>
                </div>
                <div class="search-results">
                  <div class="has-results">
                    <h1 class="search-results-title"><span class='search-results-count'></span> results matching "<span class='search-query'></span>"</h1>
                    <ul class="search-results-list"></ul>
                  </div>
                  <div class="no-results">
                    <h1 class="search-results-title">No results matching "<span class='search-query'></span>"</h1>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
      <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
          gitbook.page.hasChanged(
            {
              "page":
              {
                "title":"",
                "level":"1.1",
                "depth":1,
                "next":{},
                "dir":"ltr"
              },
              "config":
              {
                "gitbook":"*",
                "theme":"default",
                "variables":{},
                "plugins":[],
                "pluginsConfig":
                {
                  "highlight":{},
                  "search":{},
                  "lunr": {
                    "maxIndexSize":1000000,
                    "ignoreSpecialCharacters":false
                  },
                  "sharing": {
                    "facebook":true,
                    "twitter":true,
                    "google":false,
                    "weibo":false,
                    "instapaper":false,
                    "vk":false,
                    "all":[
                      "facebook",
                      "google",
                      "twitter",
                      "weibo",
                      "instapaper"
                    ]},
                  "fontsettings": {
                    "theme":"white",
                    "family":"sans",
                    "size":2
                  },
                  "theme-default": {
                    "styles": {
                      "website":"styles/website.css",
                      "pdf":"styles/pdf.css",
                      "epub":"styles/epub.css",
                      "mobi":"styles/mobi.css",
                      "ebook":"styles/ebook.css",
                      "print":"styles/print.css"
                    },
                    "showLevel":false
                  }
                },
                "structure": {
                  "langs":"LANGS.md",
                  "readme":"README.md",
                  "glossary":"GLOSSARY.md",
                  "summary":"SUMMARY.md"
                },
                "pdf": {
                  "pageNumbers":true,
                  "fontSize":12,
                  "fontFamily":"Arial",
                  "paperSize":"a4",
                  "chapterMark":"pagebreak",
                  "pageBreaksBefore":"/",
                  "margin": {
                    "right":62,"left":62,"top":56,"bottom":56
                  }
                },
                "styles": {
                  "website":"styles/website.css",
                  "pdf":"styles/pdf.css",
                  "epub":"styles/epub.css",
                  "mobi":"styles/mobi.css",
                  "ebook":"styles/ebook.css",
                  "print":"styles/print.css"
                }
              },
              "file": {
                "path":"",
                "mtime":"",
                "type":"markdown"
              },
              "gitbook": {
                "version":"3.2.3",
                "time":"2019-07-08T15:24:30.549Z"
              },
              "basePath":".",
              "book": {"language":""}});
        });
    </script>
    </div>
    <script src="https://code.jquery.com/jquery-3.5.1.min.js"></script>
    <script src="gitbook/gitbook.js"></script>
    <script src="gitbook/theme.js"></script>
    <script src="gitbook/gitbook-plugin-search/search-engine.js"></script>
    <script src="gitbook/gitbook-plugin-search/search.js"></script>
    <script src="gitbook/gitbook-plugin-lunr/lunr.js"></script>
    <script src="gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
    <script src="gitbook/gitbook-plugin-sharing/buttons.js"></script>
    <script src="gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
  </body>
</html>
